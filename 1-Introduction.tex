\section{Introduction}

The use of machine learning in real-world applications has recently seen a rise in popularity. In particular, it is becoming increasingly clear that the connectionist approach of machine learning has the potential to solve many tasks we thought reserved for humans. Examples of such tasks include playing Atari games at a super-human performance~\cite{mnih_atari} or defeating the world champion at the game of Go~\cite{silver_go}. Furthermore, the modern approach to machine learning has demonstrated that it can greatly contribute to solving the problem of predicting protein folding~\cite{jumper_alphafold}, thus potentially enabling major advances to health science and engineering. 

Despite the impressive feats of machine learning and its many promises, we need to address some problems thoroughly if we ever want to see it applied in impactful applications such as medical applications or robotic applications involving humans. One of such issues, at the center of this essay, is the problem of robustness. It has been demonstrated that learned models are very brittle~\cite{goodfellow_explaining}. Particularly when applied to data that are out-of-distribution of the data used during the training of the model itself. Out-of-distribution data can come from mainly two angles. They may arise naturally at test time due to a shift in the distribution. 
For example, think of a model trained to translate french words spoken by Frenchs in Paris used to translate the same french words spoken by Québécois from les Îles-de-la-Madelaine~\footnote{They proudly have a very thick accent: \href{https://www.youtube.com/watch?v=-16wDtOTPGs}{https://www.youtube.com/watch?v=-16wDtOTPGs}.}.
The other angle is due to an adversary trying to trick the system. Examples of applications vulnerable to such an attack: email spam filtering, anti-virus software, image classification, applications used in the Internet Of Things, etc.

Robustness is a necessary condition for the broad adoption of applications related to our security, safety and privacy using machine learning at their core. We cannot expect us to use an application that has unexpected behaviours or that can be manipulated by a third-party, especially if such an application can have grave repercussions on us. The potentially harmful impact of machine learning is one of the reasons that motivate the study of robustness in this discipline.

Game theory is a natural framework for analyzing the adversarial setting of robustness in machine learning, often referred to as Adversarial Machine Learning. Many authors produced several results giving guarantees given specific setups as well as specific algorithms using game theory. We will review some of these results. In particular, we will start with a coarse background on machine learning in Section~\ref{sec-background}. We will follow with a background on adversarial machine learning followed with a review of different equilibrium results considering the following games: simultaneous games, Stackelberg games and Bayesian games (Section~\ref{sec-review}). We will end with a discussion of the potential avenues of improvement of adversarial machine learning followed by how all of this may inform us about out-of-distribution generalization more generally (Section~\ref{sec-forward}).