\section{Adversarial machine learning}
\label{sec-review}

Adversarial machine learning is concerned with learning a program that is to be robust to adversarial attacks~\cite{aml_book}. The range of possible learning programs and adversarial attacks is broad. For example, we may consider learning a spam detector robust to an adversary that may \textit{attack} the system is such a way that the program detects its spam as non-spam. Alternatively, a program that provides personalized suggestions may have learned sensible information about its users. An adversary may have an incentive to attack the system, such that it has access to this private information. These examples demonstrate the breadth of possible attack vectors that a program may have. Furthermore, as more of our life is digitized, bad actors have increasingly more incentives to attack and jeopardize these systems. This motivates the study of adversarial machine learning and the search for formal guarantees that we can obtain. As such, we will provide a general characterization of adversarial machine learning using a commonly used framework~\cite{aml_book}, followed by a game-theoretic view of the possible equilibrium guarantees that were formulated in the literature.

\subsection{Characterization of adversarial machine learning}
A machine learning program has numerous possible vectors of attack. Knowledge of these attack vectors are relevant for both a practitioner that design a machine learning program or a game-theorist that provides formal guarantees of a system. Examples of attack vectors include:
\paragraph{Data poisoning.} Data poisoning refers to the corrupted data provided by an attacker during the training. For example, in a spam detection program, an attacker may provide spam that they labelled as non-spam. A consequence of such an attack is that the decision boundary may be sub-optimal due to poisoned data.
\paragraph{Exploitation of the structure.} When choosing the model and the objective used to train a program, choices about the structure are necessary made. These choices can almost always be exploited. For example, in a maximal-margin classifier, an attack could leverage less relevant features to force a mis-classification. A practical consequence of this effect is the adversarial perturbation of the data that can yield erroneous classification (e.g. Figure~\ref{fig:adv_class}). In this example, the attacker exploited the fact that deep neural networks are brittle
\paragraph{Exploitation of prediction error} After training, a program may make classification error. Given access to the learned program, these error may be discovered and then exploited.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{adversarial_example.png}
    \caption{Example of an adversarial example~\cite{goodfellow_explaining}. The classifier correctly classify the panda (left). But, when adding some random noise (middle), the classifier erroneously classify the panda as a gibbon (right).}
    \label{fig:adv_class}
\end{figure}

We can qualitatively encapsulate the possible attacks under a taxonomy of attacks against machine learning program. The taxonomy is comprised of three properties: Influence, Security violation and Specificity. Each of these property has a categorization that can be used to describe the attack.
\paragraph{Influence} Describes the capacity of the attacker. The influence can be decomposed into two categories:
\begin{itemize}
    \item \textbf{Causative}: The attacker has the ability to poison the data during training.
    \item \textbf{Exploratory}: The attacker has the ability to exploit the predictions, but cannot affect the training.
\end{itemize}
\paragraph{Security violation} Indicates the type of security violation that an attacker can cause. The security violation is three-fold:
\begin{itemize}
    \item \textbf{Integrity} of the system and its assets.
    \item \textbf{Availability} of the system and robustness to denial of service.
    \item \textbf{Privacy} and protection of the data used to train the system.
\end{itemize}
\paragraph{Specificity} Refers to how broad the attack is. We define two category:
\begin{itemize}
    \item \textbf{Targeted} refers to attacks that focus on a particular instance.
    \item \textbf{Indiscriminate:} Refers to an attack that broadly affect multiple instances.
\end{itemize}

While this taxonomy allows to broadly describe an attack, the litterature analyzing adversarial machine learning games rarely make assumptions pertaining to the different type of security violation or specificity of an attack. However, they do differentiate between causative and exploratory attacks, albeit implicitely. The works that we will see in the first part of the next section are concerned with simultaneous games. In these games, it is assumed that the adversary can modify the data of the learner. This is akin of the causative attack presented above. In the second part of the next section, we will present Stackelberg games, which are games alternating between each player. Generally, it is assumed that the leaner plays first and the adversary plays second since it is more representative of a real scenario. In this case, the adversary has access to the learned learner, but cannot modify the data during its training, as it is done in an exploratory attack.

Now that we have described a framework to view adversarial machine learning, we will see how the problem can be viewed as a game theoretical problem.

\subsection{Game theoretical view of adversarial machine learning}

Now that we have described what is adversarial machine learning, we can see how this problem has been framed as a game theoretic problem. We will assume two players: the learner and the adversary. The role of the adversary is to achieve one of the security violation defined above (e.g. break the integrity/availability of the system or obtain private information). The role of the learner is to perform a task while being robust to an adversary. Formally, we define a learner $f:\mathcal X\to \mathcal Y$ learned by solving an optimization problem similarly to what was defined in Equation~\eqref{eqn-learner}. We will denote the objective function of the learner $\mathcal{L}_f$. The adversary is defined as a generative model $g:\mathcal X\to \mathcal X$ which is also learned by solving an optimization problem. We will write the objective function of the adversary $\mathcal{L}_g$. The exact definition of both the learner and the adversary as well as their objective function depends on the precise problem formulation.

There are multiple ways to frame and analyze adversarial machine learning. We will first view it as a \textbf{simultaneous game}. Next, we will consider it as a \textbf{Stackelberg game} which is defined as a sequential game where one player is the \textit{leader} (plays first) while the other player is the \textit{follower} (plays second). Finally, we review \textbf{Bayesian game} formulation in which we assume that the learner does not have knowledge of the objective of the adversary. In all of those instances, we assume that the players are rational.
\subsubsection{Simulatenous game} 
In a simultaneous game, the two agents play their move at the same time. We assume that each player have knowledge of the other's objective function. In general, adversarial machine-learning is modeled as a non-cooperative game. That is, the game, represented in normal form, is given by $(N, A, U)$ where $N$ is the set of players, $A$ is the set of actions $A_i$ that a player $i$ can take and $U$ is the set of utilities $U_i$ that each player can have given their action and the actions of the other players. A player's strategy set $S_i=\{\pi(A_i):\pi(A_i)\geq 0~\forall i, \sum_{A_i}\pi(A_i)=1\}$ defines a probability distribution over its action. In this setup, we define a Nash Equilibrium a point where all the players have reached a best response strategy $s_i$ in which they cannot improve their utility further. Formally, a Nash Equilibrium is defined as follows~\cite{roughgarden} \[U(s_i, s_{-i})\geq U(s_i',s_{-i})~\forall i\in N.\]
In our setup, the set of strategies correspond to the action taken by each of our agent defined by the respective parameterized functions $f_\theta$ and $g_\phi$ for the learner and the adversary respectively. The utility itself can be defined as the inverse of the loss function $\mathcal{L}$. Therefore, an alternative view of a Nash equilibrium in our setup with one adversary and one learner is
\[
\mathcal{L}_f(\mathcal{D},f_\theta,g_\phi)\leq\mathcal{L}_f(\mathcal{D},f_{\theta'},g_\phi) \text{ and } \mathcal{L}_g(\mathcal{D},f_\theta,g_\phi)\leq\mathcal{L}_g(\mathcal{D},f_\theta,g_{\phi'}).
\]
Generally, the adversary interacts with the learner by modifying its data. Formally, considering the above loss function, we can define this interaction as follows
\[
    \mathcal{L}_f(\mathcal{D},f_\theta,g_\phi):= E_{(\bm x, y)\sim\mathcal D}\left[\mathcal{R}(f_\theta(g_\phi(\bm x)), y) + \Omega(\theta)\right].
\]


Non-cooperative games can be categorized as \textbf{zero-sum} games or general \textbf{non-zero} sum games. As such, adversarial machine learning as been framed as both these type of games. We will present a review of the litterature presenting adversarial machine learning as both zero-sum and non-zero sum simultaneous games.

\paragraph{Zero-sum games.} In 2-players zero-sum games, the objective of each player is diametrically opposed. In other words, each player has the same objective, except that one minimizes it, while the other maximizes it. More formally, for our adversarial learning setup,  $\mathcal{L}_f=\mathcal{L}_g=\mathcal L$ and
\[
\max_{\phi}\min_{\theta}\mathcal{L}(\mathcal{D},f_{\theta},g_\phi) = \min_{\theta}\max_{\phi} \mathcal{L}(\mathcal{D},f_\theta,g_\phi).
\]
In zero-sum games, if $f$ and $g$ are linear function, a Nash Equilibrium exists and it can be computed using the mini-max theorem~\cite{roughgarden}. Several works considered this formulation to propose methods to find a Nash equilibrium given different assumptions.

An assumption that may be considered is that the adversarial data lie in a certain confidence interval~\cite{Ghaoui_robust_classification,Lanckriet_robust_minimax}. For example, consider the data points with $\bm x\in\mathbb{R}^d$ and $\bm\sigma\in\mathbb{R}^d$ their interval of confidence. Consider the empirical training set $X$ with $\Sigma$ the intervals for the set of $N$ points, both defined as a $d\times N$ matrix. Finally, consider $\rho$ the sensitivity of the region. We can define a hyper-rectangle defining the region where the generator can generate points:
\[
\mathcal{Z}(\rho) = \{Z\in\mathbb{R}^{d\times N}: X-\rho\Sigma\leq Z\leq X+\rho\Sigma\}.
\]
We assume that the generator is a linear map $g_\phi:X\times\Sigma\to\mathcal{Z}(\rho)$.
%Assuming that the learner $f_\theta$ is also a linear map, the objective function is
%\[
%\min_\theta\max_{\phi}\mathcal{L}(X, \Sigma, y, f_\theta, g_\phi).
%\]
$\mathcal{L}$ can be defined as a well behaved loss, for example the logistic regression, the Hinge loss, etc. This method render a classifier robust to any perturbation within the trust-region~\cite{Ghaoui_robust_classification}.

Others have considered a generator that corrupts or deletes certain features~\cite{Globerson_robust_deletion,dekel_corrupted_learning}. In the setup where the generator delete certain features, assume that the generator masks cetain features of $\bm x$. That is, $g_m:\bm x\mapsto \bm x\odot \bm m$ with $m_i\in\{0,1\}~\forall i$ being a mask to feature $x_i$ and $\odot$ defined as the element-wise product. We have to constraint the amount of features that the adversary can mask, otherwise, it could mask all of the features and the learner could not leverage any useful information. A proposed solution~\cite{Globerson_robust_deletion} is to constraint $\sum_i m_i = K$ where $K$ is a hyper-parameter defining the number of features that the adversary should keep. Another proposition~\cite{dekel_corrupted_learning} is to consider the parameters $\theta$ of the learner as having \textit{price} defined as its value $\theta_i$. In this case, we allow a budget $P$ to the adversary, where for each parameter that the adversary masks, it has to pay a price such that the value of all the parameters it masks is lower or equal to the budget. In both a cases, the objective of the adversary is to corrupt the features in order to maximize the loss of the learner and the learner's objective is to be robust to the worst-case corruption. Moreover, both techniques can be implemented using a Linear Programming method.

\paragraph{Non-zero sum games} It can be noted that zero-sum games are overly pessimistic for adversarial machine learning. Intuitively, the objective of the adversary is rarely diametrically opposed to the one of the learner. For example, in a spam detection game, the objective of the adversary is to induce its spam as false-positive. But, the adversary is indifferent to whether the spam detector correctly classifies true-positive. The consequence of choosing an objective that is overly pessimistic is that we may incur a learner with inferior performance. For example, a spam detector may be more prone to classify a message as spam and in turn increase the number of false-positive. Therefore, general games are often a better choice to characterize adversarial machine learning games, although PSNE may not always exist and/or may be hard to compute. We have to consider two potentially different risk for the learner and the adversary that we will denote $\mathcal{R}_f$ and $\mathcal{R}_g$ respectively. We also denote the regularizers for the learner and the adversary $\Omega_f$ and $\Omega_g$ respectively. Their objective functions are \[\mathcal{L}_f(\mathcal{D}, f_\theta, g_\phi)=\mathcal{R}_f(\mathcal{D},f_\theta,g_\phi) + \lambda_f\Omega_f(\theta)\] and \[\mathcal{L}_g(\mathcal{D},f_\theta,f_\phi)=\mathcal{R}_g(\mathcal{D},f_\theta,g_\phi)+\lambda_g\Omega_g(\mathcal{D},g_\phi)\] for the learner and the adversary respectively. The role of the regularizer is usually to constraint the parameter of the learner. An example of such a regularizer is given in~\eqref{eqn-reg-learner}. For the adversary, the regularizer ensures that it cannot modify the data distribution arbitrarily. An example of such a regularizer is
\[
\Omega_g(\mathcal{D}, g_\phi) = \dfrac{1}{n}\sum_{i=1}^n||\bm x_i - g_\phi(\bm x_i)||^2,
\]
where the adversary is penalized for perturbing the data in such a way that such the data is \textit{far}, where the distance is usually defined in the feature space of the data.

This more general setup is amenable to games with more than two players. For example, there could exist multiple adversaries. But, without loss of generality, we will keep considering two players, the learner and the adversary, for simplicity. More importantly, it is possible to show the existence and, given some assumptions, the uniqueness of Mixed Nash Equilibrium~\cite{bruckner_static_aml,dritsoula_game_ml}. A Mixed Nash Equilibrium is a set of strategies, i.e. the parameters of the learner and of the adversary in our setup, sampled from distributions $\pi_\theta$ and $\pi_\phi$ respectively, such that
\[
E_{\theta\sim\pi_\theta,\phi\sim\pi_{\phi}}\mathcal{L}_f(\mathcal{D},f_\theta,g_\phi)\leq E_{\theta' \sim\pi_{\theta'}, \phi\sim\pi_\phi}\mathcal{L}_g(\mathcal{D},f_{\theta'}, g_\phi),
\]
\[ 
E_{\theta\sim\pi_\theta,\phi\sim\pi_\phi}\mathcal{L}_f(\mathcal{D},f_\theta, g_\phi)\leq E_{\theta \sim\pi_\theta, \phi'\sim\pi_{\phi'}}\mathcal{L}_g(\mathcal{D},f_\theta, g_{\phi'}).
\]


Given the following assumptions, a unique Mixed Nash Equilibrium exists~\cite{bruckner_static_aml}:
\begin{itemize}
    \item The loss functions $\mathcal{L}_f$ and $\mathcal{L}_g$ are convex and twice continuously differentiable;
    \item The regularizers $\Omega_f$ and $\Omega_g$ and uniformly strongly convex and twice continuously differentiable;
    \item The set of free-parameters of the functions $f$ and $g$ are non-empty, compact, and convex subset of finite-dimensional Euclidean spaces.
\end{itemize}

We will now consider non-competitive games in a sequential game framework.

\subsubsection{Stackelberg game} A slightly different framework for describing adversarial machine learning games are the sequential games (i.e. Stackelberg game). In this setup, each player takes turn in selecting their strategy. The first player is called the leader while the second player is called the follower. When taking its action, the follower has knowledge of the leaders strategy profile. While we could theoretically alternate the follower and the leader as in a mouse and cat game~\cite{barreno_security_ml}, we are interested in finding an equilibrium in which both the learner and the follower are stable. Depending on the which player is the leader or the follower, we may have different Stackelberg equilibrium. The case where the leader is the adversary and lassifielearnerr has been considered in some work~\cite{murat_adversarial_classification,liu_game_theoretical_aml}. While interesting, it is not practically realistic for the problem of robustness to adversarial machine learning, because, in this case, the adversarial data are essentially in-distrubtion for the learner. We will instead consider the more realistic case where the leader is the learner and the follower is the adversary~\cite{bruckner_stackelberg}. We will also assume that the follower (i.e. the adversary) will play rationally according to its objective function and that the learner has knowledge about the objective function of the adversary. For example, a learner classifying spams would know that the objective of the adversary is to maximize the number of time it fools the learner to classify its spams as non-spam.

More formally, we assume that the adversary is learning a transformation $g$ given an optimal set of parameters $\theta'$ for the learner as follows:
\begin{equation}
\label{eqn-stackle_gen}
    \min_\phi \mathcal{L}_g(\mathcal{D}, f_{\theta'}, g_\phi).
\end{equation}
This amount to solving a regular optimization problem because $f_{\theta'}$ is known at the time of the optimization.

Assuming that the adversary will solve the above optimization problem to choose its $\phi'$ that minimize its objective function, the learner chooses the model $f$ that minimizes its objective function $\mathcal{L}_f$.
\[
\min_\theta \mathcal{L}_f(\mathcal{D}, f_\theta, g_{\phi'}),
\]
Further, the optimal $\phi'$ might not be unique. In fact, a set $\phi'\in\Phi'$ might solve Equation~\eqref{eqn-stackle_gen}. In this case, two views may be possible: the optimist and the pessimist view. For example, if we assume the worst case when learning the model parameters $\theta$, we may solve the following optimization problem:
\[
\min_\theta\max_{\phi'\in\Phi'} \mathcal{L}_f(\mathcal{D}, f_\theta, g_{\phi'}).
\]
A Stackleberg equilibrium can be identified by backward induction: the learner chooses $\theta'$ that maximizes its utility with respect to the attacker’s response $g_{\phi'}$. More specifically, the equilibrium can be found by solving the following bilevel optimization problem where the objective of the learner is the outer loop and the objective of the adversary is the inner loop. Assuming a unique $g_{\phi'}$:
\begin{equation}
    \label{eqn-bilevel}
    \begin{split}
        &\min_\theta \mathcal{L}_f(\mathcal{D}, f_\theta, g_{\phi'})\\
        & \text{ s.t. } \phi' = \arg\min_{\phi}\mathcal{L}_g(\mathcal{D}, f_\theta, g_{\phi'}).
    \end{split}
\end{equation}
Bilevel optimization are NP-hard to solve exactly, even in cases where the objectives are linear~\cite{jeroslow_polynomial_bilevel}. Different method exists to relax the problem, such as relying on greedy search method like gradient descent~\cite{naveiro_gradient_stackelberg} or adding regularizers, at the cost of losing the optimality guarentees~\cite{colson_bilevel}.

It is possible to solve the above problem by constraining further the problem and considering a special case of the bilevel optimization. Assume that the regularizers $\Omega_f$ and $\Omega_g$ are strongly convex, that the risk of the adversary $\mathcal{R}_g$ is convex and that $f_\theta:X\to\mathbb{R}$ is linear. Further, assume that the inner optimization problem is replaced with its Karush-Kuhn-Tucker conditions. We can do so by considering a set of constraints $\bm\tau$ defined in the inner optimization problem of the bi-level optimization problem as shown in the following:
\begin{equation}
    \label{eqn-convex-stack}
    \begin{split}
    &\min_{\theta}, \dfrac{1}{n}\sum_{i=1}^n\mathcal{R}_f(f_\theta(x_i) + \tau_i||\theta||^2, y_i) + \dfrac{\lambda_f}{2}||\theta||^2\\
    &\text{ s.t. } \forall~i: 0=\tau_i + \dfrac{n}{\lambda_g}\mathcal{R}_g(f_\theta(x_i) + \tau_i||\theta||^2, y_i).
    \end{split}
\end{equation}
The Stackelberg prediction game from Equation~\eqref{eqn-convex-stack} attains an equilibrium where $g_{\bm\tau}':\bm x_i\to \bm x_ + \tau_i\bm w$~\cite{bruckner_stackelberg}. The solution to this problem can be obtained by a sequential quadratic programming method.

While this analytic formulation is very contrived, it is possible to use numerical approaches to approximate a solution. One way to approximate Equation~\eqref{eqn-bilevel} is by following a PDE-constrained optimization problem~\cite{naveiro_gradient_stackelberg}. Assume $f_\theta$ and $g_\phi$ linear, and that the inner problem of Equation~\eqref{eqn-bilevel} has a unique solution $g_\phi'$ for each $f_\theta$. We define the following differential equation $G:\theta\times t\to \phi$, where $t\in\mathbb{R}_+$ is a time variable. In can be shown under mild condition that the following constrained optimization problem converges with $G(\theta,t)\to \phi'$ as $t\to\infty$ with rate $\mathcal O(\dfrac{1}{t})$~\cite{naveiro_gradient_stackelberg}:
\begin{equation*}
    \begin{split}
        &\min_\theta \mathcal{L}_f(\mathcal{D}, f_\theta, g_{G(\theta,T)})\\
        \text{ s.t. } &\partial_tG(\theta, t) = \partial_G\mathcal{L}_g(\mathcal{D}, f_\theta, g_{G(\theta, t)})\\
        & G(\theta, 0) = 0.
    \end{split}
\end{equation*}


\subsubsection{Bayesian game}
So far, we assumed that each player had the knowledge about the other player objective function.
However, in practice, such an assumption may be hard to meet.
For example, we may  not know that the objective of the adversary is to send spam related to a particular product, for example car insurances.
In such a case, we want to relax the assumption that both players has knowledge about the other's objective and consider non-cooperative games with incomplete information.
Instead, we consider that the players have \textit{beliefs} about the other players objective function. These beliefes are represented by a probability distribution and are updated using the Bayes rule.
This framework is usually referred as the Bayesian game~\cite{harsanyi_bayesian_games}. A Bayesian game is similar to a Normal-form game, with the exception that each player can have a type $T_i$.
The type of a player $i$ is sampled from a space $\mathcal{T}$, according to a probability distribution $p(\mathcal{T})$. The utility $u_i:A_i\times T_i\to\mathbb{R}$ maps from the product of the action of
an agent $i$ and its type $T_i$ to a scalar.

A Bayesian Nash equilibrium is also similar to the one of a Normal game, with the difference that a Bayesian Nash equilibrium is defined as a strategy profile that maximizes the expected utility of each player
given their \textit{beliefs} $p(T_{-i}|T_i)$ of the other players type and given the stratgies played by the other players. In our adversarial machine learning setup, we represent the expected objective the learner $f$
given a single adversary $g$ in a Bayesian game as follows
\[
     E\mathcal{L}_f(\mathcal{D}, f_\theta, g_\phi, T_f) = \sum_{T_g\in\mathcal{T}}p(T_g|T_f)\mathcal L_f(\mathcal{D}, f_{\theta}^{T_f}, g_{\phi}^{T_g}),
\]
and similarly for the adversary
\[
    E\mathcal{L}_g(\mathcal{D}, f_\theta, g_\phi, T_g) = \sum_{T_f\in\mathcal{T}}p(T_f|T_g)\mathcal L_g(\mathcal{D}, f_{\theta}^{T_f}, g_{\phi}^{T_g}).
\]
Given the above definition of expected utility, we can define Pure Nash equilibrium:
\[
    E\mathcal{L}_f(\mathcal{D}, f_{theta}, g_\phi, T_f) \leq E\mathcal{L}_f(\mathcal{D}, f_{\theta'}, g_{\phi}, T_f)~\forall~\theta',
\]
\[
    E\mathcal{L}_g(\mathcal{D}, f_\theta, g_\phi, T_g) \leq E\mathcal{L}_g(\mathcal{D}, f_\theta, g_{\phi'}, T_f)~\forall~\phi'.
\]
If we had $p(T_f|T_g)$ and $p(T_g|T_f)$, computing the Nash equilibrium for Simulatenous and Stackelberg game is no different, with the exception
of the sum over $\mathcal{T}$ which can be arbitrarily large. The issue arise if we do not have the beliefs. In such a case,
we have to compute it using Bayes' theorem:
\[
    p(T_f|T_g) = \dfrac{p(T_g|T_f)p(T_f)}{p(T_g)}.
\]
In a realistic scenario, we may know not know the prior of a player, for example the adversary. In this case, we can compute it
by marginalizing the joint distribution $p(T_g|T_f)p(T_f)$. But, the issue is that for an arbitrary distribution, such computation may be intractable.
Therefore, method like sampling or variational approximation may be used.
